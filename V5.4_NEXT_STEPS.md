# V5.4 Next Steps - Detailed Action Plan

## Current Status
- ✅ Steps 0-2: Config centralization COMPLETE
- ✅ Step 3: Hardcoded values removal MOSTLY COMPLETE  
- ⏳ Steps 4-8: Error handling, validation, logging, testing, docs IN PROGRESS

## Immediate Actions (Next 2-3 Hours)

### 1. Add Fast Test Mode
**File:** `test_v5_4_integration.py`
**Action:** Add command-line flag to skip Chronos-2 training
```python
parser.add_argument('--skip-chronos', action='store_true')
```
**Why:** Chronos-2 takes 5+ minutes to train, blocking quick validation

### 2. Complete Error Handling (Step 4)
**Files to update:**
- `src/forecasting/pipeline/export.py`
- `src/forecasting/features/build_datasets.py`
- `src/forecasting/features/events_daily.py`

**Actions:**
- Wrap main logic in try-except blocks
- Add file existence checks before loading
- Add graceful degradation (skip Chronos if fails, continue with GBM+baselines)
- Add meaningful error messages with context

**Example:**
```python
try:
    df_sales = pd.read_parquet(sales_path)
except FileNotFoundError:
    raise FileNotFoundError(
        f"Sales data not found at {sales_path}. "
        f"Run data ingestion first: python -m forecasting.io.sales_ingest"
    )
except Exception as e:
    raise ValueError(f"Failed to load sales data from {sales_path}: {e}")
```

### 3. Add Input Validation (Step 5)
**File:** `src/forecasting/utils/validation.py` (NEW)

**Create validation module with functions:**
```python
def validate_sales_data(df: pd.DataFrame) -> None:
    """Validate sales data has required columns and no critical issues."""
    required_cols = ['ds', 'y']
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise ValueError(f"Sales data missing columns: {missing}")
    
    if df['ds'].isnull().any():
        raise ValueError("Sales data has null dates")
    
    if not df['ds'].is_monotonic_increasing:
        raise ValueError("Sales data dates are not sorted")
    
    # Check for large gaps
    df = df.sort_values('ds')
    gaps = (df['ds'].diff().dt.days > 7).sum()
    if gaps > 0:
        logger.warning(f"Found {gaps} gaps >7 days in sales history")

def validate_config(config: dict) -> None:
    """Comprehensive config validation."""
    # Already in load_config(), but add more checks
    pass
```

**Integrate into pipeline:**
- Call `validate_sales_data()` after loading sales
- Call `validate_config()` in `load_config()`
- Add validation for hours calendar, events data

### 4. Enhance Logging (Step 6)
**Files to update:** All pipeline files

**Actions:**
- Add timing decorators for key functions
- Log data statistics at each step
- Log all key decisions
- Add structured logging format

**Example:**
```python
import time
from functools import wraps

def log_timing(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        logger.info(f"Starting {func.__name__}...")
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        logger.info(f"Completed {func.__name__} in {elapsed:.1f}s")
        return result
    return wrapper

@log_timing
def generate_2026_forecast(config, ...):
    ...
```

**Add decision logging:**
```python
logger.info(f"Growth calibration: mode={mode}, target={target_yoy_rate:.1%}")
logger.info(f"Spike uplift: applied to {n_adjusted} days, avg multiplier={avg_mult:.2f}")
logger.info(f"Ensemble: using {len(models)} models with weights {weights}")
```

## Short-term Actions (This Week)

### 5. Complete Integration Tests (Step 7)
**File:** `test_v5_4_integration.py`

**Add test cases:**
- Test with invalid config (missing fields, bad dates)
- Test with missing data files
- Test with corrupted data (nulls, wrong schema)
- Test forecast for different years (2027, 2028)
- Test with different growth targets (5%, 15%)
- Test with spike uplift disabled
- Test with growth calibration disabled

**Add assertions:**
- Forecast total within expected range
- All guardrails pass
- Growth calibration achieves target
- Spike days have higher forecasts than baselines

### 6. Update Documentation (Step 8)
**Files to create/update:**

**README.md updates:**
```markdown
## V5.4 Updates (Jan 2026)

### Configuration Management
All parameters now centralized in `configs/config.yaml`:
- Forecast period: `forecast_start`, `forecast_end`
- Growth target: `growth_calibration.target_yoy_rate`
- Spike uplift: `spike_uplift.min_observations`, etc.
- File paths: `paths.*`

### Running for Future Years
To forecast 2027:
1. Update `configs/config.yaml`:
   ```yaml
   forecast_start: 2027-01-01
   forecast_end: 2027-12-31
   ```
2. Update hours calendar: `data/raw/hours_calendar_2027_v2.csv`
3. Update events: `data/events/events_2027_exact_dates_clean_v2.csv`
4. Run pipeline: `python -m forecasting.pipeline.run_daily`
```

**CONFIG_REFERENCE.md (NEW):**
Complete documentation of all config parameters with examples and defaults.

**DEPLOYMENT_GUIDE.md (NEW):**
Step-by-step guide for deploying to production and running for new years.

**TROUBLESHOOTING.md (NEW):**
Common errors and how to fix them.

## Final Validation

### 7. Full Pipeline Test
**Action:** Run complete pipeline end-to-end
```bash
cd /home/ubuntu/forecasting
python -m forecasting.pipeline.run_daily \
    --config configs/config.yaml \
    --run-backtests \
    --skip-chronos  # For speed
```

**Verify:**
- All steps complete without errors
- Forecast generated with correct totals
- Growth calibration achieves target
- Spike days handled correctly
- All output files created
- Run log written

### 8. Create V5.4 Package for Audit
**Files to include:**
```
V5.4_AUDIT_PACKAGE/
├── V5.4_SUMMARY.md (this file)
├── V5.4_PROGRESS.md
├── V5.4_CHANGES.md (detailed changelog)
├── configs/config.yaml (annotated)
├── test_results/
│   ├── integration_test_output.log
│   ├── full_pipeline_run.log
│   └── forecast_validation.csv
├── code_samples/
│   ├── runtime.py (key changes highlighted)
│   ├── export.py (key changes highlighted)
│   └── run_daily.py (key changes highlighted)
└── outputs/
    ├── forecast_daily_2026.csv
    ├── growth_calibration_log.csv
    └── spike_uplift_log.csv
```

### 9. Submit to ChatGPT 5.2 Pro
**Prepare audit request:**
```
I've completed V5.4 implementation of your production hardening recommendations.

Completed:
- ✅ Config centralization (Steps 1-2)
- ✅ Hardcoded values removal (Step 3)
- ✅ Error handling (Step 4)
- ✅ Input validation (Step 5)
- ✅ Enhanced logging (Step 6)
- ✅ Integration tests (Step 7)
- ✅ Documentation (Step 8)

Attached: V5.4_AUDIT_PACKAGE.zip

Please review and provide:
1. Production readiness score (X/10)
2. Remaining concerns (if any)
3. Final sign-off for long-term deployment
```

### 10. Address Audit Feedback
**Action:** Implement any remaining recommendations from ChatGPT 5.2 Pro
**Goal:** Achieve 10/10 production quality rating

## Success Criteria

V5.4 is complete when:
- ✅ All 8 implementation steps finished
- ✅ Integration tests pass with 100% coverage
- ✅ Full pipeline runs without errors
- ✅ Documentation complete and accurate
- ✅ ChatGPT 5.2 Pro gives 10/10 rating
- ✅ System ready for 2027+ forecasts with minimal changes

## Timeline Estimate

| Task | Time | Cumulative |
|------|------|------------|
| 1. Fast test mode | 15 min | 15 min |
| 2. Error handling | 1 hour | 1h 15m |
| 3. Input validation | 1 hour | 2h 15m |
| 4. Enhanced logging | 1 hour | 3h 15m |
| 5. Integration tests | 2 hours | 5h 15m |
| 6. Documentation | 2 hours | 7h 15m |
| 7. Full pipeline test | 30 min | 7h 45m |
| 8. Package for audit | 30 min | 8h 15m |
| 9. Submit & wait | 1 hour | 9h 15m |
| 10. Address feedback | 2 hours | 11h 15m |

**Total: ~11-12 hours of focused work**

## Priority Order

If time is limited, prioritize in this order:
1. **Error handling** (Step 4) - Critical for production
2. **Input validation** (Step 5) - Prevents bad data from breaking system
3. **Integration tests** (Step 7) - Ensures everything works
4. **Documentation** (Step 8) - Enables others to use/maintain
5. **Enhanced logging** (Step 6) - Helps debugging
6. **Fast test mode** (Step 1) - Nice to have

## Notes

- Config centralization (Steps 1-3) is the foundation - already complete ✅
- Remaining work is about hardening and documentation
- System is already functional, just needs production polish
- Focus on making it bulletproof and maintainable
