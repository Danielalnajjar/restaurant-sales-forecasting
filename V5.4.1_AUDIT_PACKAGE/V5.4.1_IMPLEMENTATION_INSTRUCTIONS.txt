Below is a copy‑paste ready prompt you can give to Manus. It’s written so Manus can implement mechanically (minimal/no reasoning), with exact files, exact edits, and concrete code snippets.

Use this as the single instruction Manus should follow end‑to‑end.

===============================================================================
MANUS IMPLEMENTATION PROMPT — “V5.4 → V5.4.1 (10/10 Quality Hardening)”
===============================================================================

<task_spec>
Goal:
Bring the Las Vegas restaurant forecasting repo to true “10/10 production quality” by fixing ALL remaining technical debt / edge-case bugs in V5.4, tightening year-agnostic behavior, eliminating duplicate artifacts, and making tests + validation deterministic and CI-friendly — WITHOUT changing the numerical 2026 forecast (numeric parity must remain).

Definition of done (MUST ALL PASS):
1) ✅ No regressions in 2026 forecast values:
   - Running the pipeline for the 2026 window produces the exact same forecast values as the current V5.4 baseline (max abs diff for p50/p80/p90 = 0.0000).
   - Use the existing compare script (or write one if needed) and save a “parity report”.

2) ✅ Pipeline runs from clean checkout with ONE command:
   - `python -m forecasting.pipeline.run_daily --config configs/config.yaml`
   - exits 0 and writes outputs for the configured forecast window.

3) ✅ Zero hidden bugs in spike uplift priors computation:
   - If spike priors file is deleted, pipeline recomputes priors without crashing.
   - No pandas boolean “not” misuse anywhere.

4) ✅ Config truly centralized:
   - Config is loaded ONCE at entry (run_daily).
   - No module opens YAML directly (no `open("configs/config.yaml")` inside feature/model code).
   - All functions accept a `config: dict` parameter passed from the top.

5) ✅ Output naming is fully year-agnostic and consistent:
   - ALL generated artifacts that are per-run must include the forecast slug (including spike logs and calibration logs).
   - A stable “latest” pointer file may exist, but must be an exact copy of the slugged artifact.

6) ✅ Validation script is correct and actually fails when it should:
   - If required outputs are missing, validate exits non-zero.
   - validate imports match runtime module API (no broken imports).

7) ✅ Tests are not “false green”:
   - Add missing unit tests to cover the known blind spots listed below.
   - `pytest` passes 100% without requiring a prior manual pipeline run (no “skip pending outputs” for core logic tests).
   - You may keep 1 optional integration test marked `@pytest.mark.slow`, but unit tests must pass in isolation.

8) ✅ Ruff + formatting:
   - `ruff check .` passes
   - `ruff format .` passes (if enabled in pyproject)
   - No blanket `except Exception: pass` that hides failures in critical logic.

Non-goals:
- Do NOT change the forecasting methodology.
- Do NOT add new external data sources.
- Do NOT tune model hyperparameters.
- Do NOT change the 2026 forecast numbers (must maintain numeric parity).
</task_spec>

<context>
Repo layout assumptions:
- Python package lives under: `src/forecasting/`
- Canonical config is: `configs/config.yaml`
- There may be legacy fallback config at: `code/config.yaml` (allowed, but canonical is configs/config.yaml)

Known V5.4 issues to fix (treat these as MUST FIX):
A) spike_uplift.py has invalid boolean filtering using `not df_open[flag]` (must be `~df_open[flag]`)
B) run_daily.py writes a separate run_log.json that can drift from export’s run_log_{slug}.json (remove duplication)
C) run_daily.run_pipeline signature defaults skip_chronos=True (should be False)
D) build_datasets.py still loads YAML internally (config not centralized)
E) events_daily.py still uses year-specific mapping columns in a hardcoded way (must be year-aware for history + forecast year)
F) scripts/validate.py has API/import issues and output checks do not fail build reliably
G) “slug-based naming” is incomplete for per-run logs (spike_uplift_log.csv and growth_calibration_log.csv should be slugged)
</context>

===============================================================================
WORK PLAN (DO IN ORDER — DO NOT SKIP STEPS)
===============================================================================

STEP 0 — Create a parity baseline (required)
1) Checkout V5.4 as-is.
2) Run:
   - `python -m forecasting.pipeline.run_daily --config configs/config.yaml`
3) Copy the generated slugged forecast to a “baseline” file to compare later, e.g.:
   - `outputs/forecasts/forecast_daily_<slug>__BASELINE.csv`
4) If a compare script exists, run it and save output. If not, create:
   - `scripts/compare_forecasts.py` that compares baseline vs current by ds and p50/p80/p90 and prints max abs diff + totals.

Acceptance: You have a baseline CSV and a way to compute numeric parity.

-------------------------------------------------------------------------------

STEP 1 — Fix spike uplift boolean bug (CRITICAL)
File: `src/forecasting/features/spike_uplift.py`

Problem:
The code uses `not df_open[flag]` inside pandas boolean indexing. This is invalid and can crash whenever priors need to be recomputed.

Implement EXACT fix:
Replace EVERY instance of:
- `(not df_open[flag])`
with:
- `(~df_open[flag])`

Also ensure the spike flag columns are boolean before masks are used:
Right after:
```python
df_open = df[df['is_closed'] == False].copy()

Add:

# Ensure spike flags are boolean for safe masking
for flag in spike_flags:
    if flag in df_open.columns:
        df_open[flag] = df_open[flag].astype(bool)

Concrete code replacement (copy/paste patch concept):
Search in compute_spike_uplift_priors(...) for these blocks and replace:

BEFORE (example):

baseline_days = df_open[(df_open['dow'] == dow) & (df_open['month'] == month) & (not df_open[flag])]

AFTER:

baseline_days = df_open[(df_open['dow'] == dow) & (df_open['month'] == month) & (~df_open[flag])]

Do the same for BOTH baseline_days assignments and the fallback baseline_days assignment.

Acceptance:
	•	Running python -m pytest -q passes (after later test updates).
	•	Deleting the priors file and rerunning pipeline does not crash.

⸻

STEP 2 — Eliminate duplicate / conflicting run logs (CRITICAL)
Goal: There must be exactly ONE source of truth for run metadata.

Rule:
	•	export.py generates the canonical slugged run log: run_log_<slug>.json
	•	A stable pointer file MAY exist (e.g., run_log.json), but it must be an exact copy of the slugged run log content created by export.py (not separately recomputed).

Edits:

2A) File: src/forecasting/pipeline/run_daily.py
Remove the entire block that manually constructs/writes run_log.json.
In V5.4-like code, this block starts with something like:

# Generate run metadata after successful completion
run_metadata = { ... }
...
with open(run_log_path, 'w') as f:
    json.dump(run_metadata, f, indent=2)

Delete that whole section.

Instead, after generate_forecast(...) returns, simply log where the run log is written (do NOT write another JSON).

2B) File: src/forecasting/pipeline/export.py
After writing the slugged run log, also write/copy a stable “latest” pointer.
Immediately after:

with open(run_log_path, "w") as f:
    json.dump(run_log, f, indent=2)

Add:

# Write a stable pointer to the latest run log (exact copy of slugged log)
run_log_latest_path = output_reports_dir / "run_log.json"
with open(run_log_latest_path, "w") as f:
    json.dump(run_log, f, indent=2)

Acceptance:
	•	Only export.py writes run metadata.
	•	outputs/reports/run_log_<slug>.json exists and outputs/reports/run_log.json exists and matches content exactly.

⸻

STEP 3 — Fix Chronos default behavior (MEDIUM but required)
File: src/forecasting/pipeline/run_daily.py

Requirement:
Chronos should be ENABLED by default; --skip-chronos disables it.

Implementation:
3A) Ensure CLI arg is:

parser.add_argument("--skip-chronos", action="store_true", help="Skip Chronos-2 model")

No default=True.

3B) Ensure run_pipeline(...) signature default is:
BEFORE:

def run_pipeline(..., skip_chronos: bool = True, ...):

AFTER:

def run_pipeline(..., skip_chronos: bool = False, ...):

Acceptance:
	•	python -m forecasting.pipeline.run_daily --help shows skip-chronos.
	•	Running without flag uses Chronos when available.
	•	Running with flag forces skip.

⸻

STEP 4 — Centralize config loading everywhere (CRITICAL)
Rule:
	•	ONLY run_daily.py calls load_config(...).
	•	No other module opens YAML.
	•	All downstream functions accept a config dict and use it.

4A) File: src/forecasting/features/build_datasets.py
Remove ANY code that does:

config_path = Path("configs/config.yaml")
with open(config_path) as f:
    config = yaml.safe_load(f)

Change function signatures to require config passed in:
	•	build_train_datasets(..., config: dict)
	•	build_inference_features(..., config: dict)

Remove any defaults like:
config.get('forecast_start', '2026-01-01')
because runtime.validate_config already requires forecast_start/end.

Replace with:

forecast_start = config["forecast_start"]
forecast_end = config["forecast_end"]

4B) File: src/forecasting/pipeline/run_daily.py
Update calls:
BEFORE:

train_short, train_long = build_train_datasets()
inference_short, inference_long = build_inference_features_2026(config)

AFTER:

train_short, train_long = build_train_datasets(config=config)
inference_short, inference_long = build_inference_features_2026(config=config)

Acceptance:
	•	grep -R "open(.*config.yaml" -n src/forecasting finds nothing except runtime utilities/tests.
	•	Only run_daily loads config.

⸻

STEP 5 — Make events mapping year-aware (CRITICAL for 2027+)
File: src/forecasting/features/events_daily.py

Goal:
	•	History features must be correct even when history spans multiple years (e.g., mid-2026 reruns).
	•	Forecast features must use mapping columns that match the configured forecast year (start_YYYY / end_YYYY), not hardcoded 2026.

Implementation requirements:
5A) Add helper to resolve mapping columns:
Add near top-level in events_daily.py:

def _mapping_cols_for_year(year: int) -> tuple[str, str]:
    return f"start_{year}", f"end_{year}"

5B) Update build_events_daily_history:
Current behavior uses start_2025/end_2025 only.
Replace with:
	•	Determine years present in history sales: years = sorted(df_sales["ds"].dt.year.unique())
	•	For each year, if mapping has start_{year}/end_{year}, expand those events into daily rows for that year
	•	Concatenate expansions across years
	•	For years not present in mapping columns, do NOT crash; log warning and skip (events = 0)

Pseudo-implementation block (copy/paste structure):

years = sorted(df_sales["ds"].dt.year.unique())
all_expanded = []

for y in years:
    start_col, end_col = _mapping_cols_for_year(int(y))
    if start_col not in df_map.columns or end_col not in df_map.columns:
        logger.warning(f"Recurring mapping missing columns for year {y}: {start_col}/{end_col}. Skipping.")
        continue

    df_y = df_map[["event_family", start_col, end_col]].copy()
    df_y = df_y.rename(columns={start_col: "start_date", end_col: "end_date"})
    df_y = df_y.dropna(subset=["start_date", "end_date"])

    # Expand into daily rows
    for _, r in df_y.iterrows():
        start = pd.to_datetime(r["start_date"])
        end = pd.to_datetime(r["end_date"])
        if pd.isna(start) or pd.isna(end) or end < start:
            continue
        dates = pd.date_range(start=start, end=end, freq="D")
        all_expanded.append(pd.DataFrame({"ds": dates, "event_family": r["event_family"], "is_active": 1}))

Then:
	•	concatenate
	•	pivot to event_family__* columns
	•	ensure full ds range from df_sales min/max present with 0 fill

5C) Update build_events_daily_forecast (currently build_events_daily_2026)
	•	Determine forecast year from forecast_start:

forecast_year = pd.to_datetime(config["forecast_start"]).year
start_col, end_col = _mapping_cols_for_year(forecast_year)

	•	Use those columns (must exist or raise a clear error):
If missing, raise:

raise ValueError(f"Recurring mapping file missing {start_col}/{end_col} for forecast_year={forecast_year}")

Acceptance:
	•	No hardcoded “start_2026”/“end_2026” usage for forecast.
	•	Forecasting 2027 works if mapping file contains start_2027/end_2027 and events_exact_file is updated.

⸻

STEP 6 — Complete slug-based naming for ALL per-run artifacts (CRITICAL)
Problem:
Some logs are currently fixed names, which breaks year-agnostic usage.

Requirement:
All per-run outputs must include slug:
	•	spike uplift log
	•	growth calibration log
	•	monthly calibration scales
	•	(optional) priors snapshot

Implementation approach:
6A) Update runtime output paths
File: src/forecasting/utils/runtime.py
In get_output_paths(config), add:

"spike_uplift_log": outputs_dir / "reports" / f"spike_uplift_log_{slug}.csv",
"growth_calibration_log": outputs_dir / "reports" / f"growth_calibration_log_{slug}.csv",
"monthly_calibration_scales": outputs_dir / "reports" / f"monthly_calibration_scales_{slug}.csv",

6B) Update export.py to pass these paths down:
File: src/forecasting/pipeline/export.py
	•	Replace hardcoded report paths with output_paths entries.

Example:
BEFORE:

spike_log_path = output_reports_dir / "spike_uplift_log.csv"

AFTER:

spike_log_path = output_paths["spike_uplift_log"]

Similarly for growth_calibration_log and monthly_calibration_scales.

6C) Write stable “latest” pointers (optional but recommended)
After writing slugged logs, write:
	•	outputs/reports/spike_uplift_log.csv as copy of spike_uplift_log_{slug}.csv
	•	outputs/reports/growth_calibration_log.csv as copy of growth_calibration_log_{slug}.csv
	•	outputs/reports/monthly_calibration_scales.csv as copy of monthly_calibration_scales_{slug}.csv

These must be exact copies (same content).

Acceptance:
	•	Running for multiple years does not overwrite slugged logs.
	•	Stable non-slug names always reflect last run.

⸻

STEP 7 — Fix validate.py so it actually validates (CRITICAL)
File: scripts/validate.py

7A) Fix runtime imports to match actual API.
Do NOT import non-existent functions.
Use:

from forecasting.utils.runtime import load_config

7B) Make output existence checks affect exit code:
Every check must contribute to all_passed.
Pattern:

all_passed &= check_file_exists(path, "desc")

7C) Validate should fail if required artifacts missing:
At minimum require:
	•	forecast_daily_{slug}.csv
	•	run_log_{slug}.json
	•	rollups_ordering_{slug}.csv
	•	rollups_scheduling_{slug}.csv
	•	ensemble_weights_{slug}.csv

Spike/growth logs can be required if the pipeline is configured to produce them.

Acceptance:
	•	Delete forecast file and run validate → exits non-zero.
	•	Everything present → exits 0.

⸻

STEP 8 — Add missing unit tests (to prevent silent future regressions)
File(s): tests/

Add these tests (exact expectations):
8A) test_spike_priors_recompute_does_not_crash
	•	Create a tiny df_hist with 30 days, include one spike day, is_closed False, and ensure compute_spike_uplift_priors(...) returns a DataFrame with that flag row and no exceptions.
	•	This test must fail if not df_open[flag] exists.

8B) test_run_pipeline_default_chronos_enabled
	•	Import run_pipeline signature and assert default skip_chronos is False.

8C) test_slugged_logs_written
	•	Using runtime.get_output_paths(config), assert keys include spike_uplift_log, growth_calibration_log, monthly_calibration_scales.
	•	If outputs exist, confirm filenames include slug.

8D) test_build_datasets_no_yaml_load
	•	Grep build_datasets.py for yaml.safe_load and open( targeting config.yaml and fail if found.

Do NOT make these tests depend on committed output files; they must be pure unit tests with temp dirs + synthetic data where needed.

⸻

STEP 9 — Ruff + formatting + remove silent failure patterns
9A) Run:
	•	ruff check . --fix
	•	ruff format .

9B) Remove any except Exception: pass in critical pipeline logic.
If an exception is intentionally non-fatal, it must:
	•	log with logger.exception(…)
	•	and set a clear field in run_log (e.g., “spike_uplift_status”: “failed”) OR re-raise if config has strict_mode enabled.

Do NOT add complex strict_mode if it risks changing behavior. If strict_mode already exists, use it. Otherwise, just log exceptions loudly.

⸻

STEP 10 — Re-run full validation + parity
10A) Run:
	•	python -m pytest -q
	•	ruff check .
	•	python scripts/validate.py
	•	Pipeline:
python -m forecasting.pipeline.run_daily --config configs/config.yaml

10B) Run parity compare:
	•	Compare the new forecast to the baseline saved in Step 0.
	•	Must show:
	•	max abs diff p50/p80/p90 = 0.0000
	•	total difference = 0.00

10C) Output a final “IMPLEMENTATION_SUMMARY.md” containing:
	•	list of files changed
	•	commands run
	•	test results
	•	parity results

===============================================================================
DELIVERABLES

When finished, produce:
	1.	A list of files changed (paths).
	2.	The exact parity report output.
	3.	pytest output summary.
	4.	validate.py output.
	5.	A brief note confirming: “10/10 quality criteria met” and why.
===============================================================================

If you want, paste Manus’s implementation diff (or the updated zip) back here and I’ll do a **final verification pass** to confirm it really hits 10/10 and that forecast parity held.